{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Train and Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = pd.read_csv(\"../input/train.csv\")\nprint(train_input.shape)\n\ntrain_X = train_input.iloc[:, :-1]\ntrain_X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_Y = train_input.iloc[:, -1]\ntrain_Y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/test.csv\")\nprint(test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"### Drop Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = train_X.drop(columns=[\"homepage\", \"imdb_id\", \"poster_path\", \"status\"])\n\ntrain_X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical to Numeric Tranformation\n#### original_language"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"ratio of ocurrences of \\\"en\\\" to all data: %f\" % (len([i for i in train_X[\"original_language\"] if i == \"en\"])/len(train_X[\"original_language\"])))\n\nunique, counts = np.unique([i for i in train_X[\"original_language\"]], return_counts=True)\nlanguage_counts = dict(zip(unique, counts))\n\nplt.figure(figsize=(18, 5))\nplt.bar(*np.unique([i for i in train_X[\"original_language\"] if i != \"en\"], return_counts=True))\nplt.show()\n\npercentage_required = 0.01\n\nlanguages = [i for i in language_counts.keys() if language_counts[i]/len(train_X[\"original_language\"]) > percentage_required]\n\nprint(\"Languages that make up at least \" + str(percentage_required * 100) + \"% of the data \" + str(languages))\nprint(\"Data % represented: \" + str(len([i for i in train_X[\"original_language\"] if i in languages])/len(train_X[\"original_language\"])))\n\none_hot_lang = pd.get_dummies(pd.Categorical(train_X[\"original_language\"]), prefix=\"language\")[[\"language_\" + i for i in languages]]\nprint(one_hot_lang.head())\n\ntrain_X = pd.concat([train_X.drop(columns=\"original_language\"), one_hot_lang], axis=1)\ntrain_X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### belongs_to_collection"},{"metadata":{"trusted":true},"cell_type":"code","source":"import ast\n\nbtc = train_X[\"belongs_to_collection\"].apply(lambda x: [i[\"id\"] for i in ast.literal_eval(x)] if type(x) == str else [])\nprint(len(np.unique(btc)))\n\n# high cardinality, will try using if it belongs to a collection or not as a feature instead\n\nbtc = train_X[\"belongs_to_collection\"].apply(lambda x: 1 if type(x) == str else 0)\nbtc.head()\n\ntrain_X[\"belongs_to_collection\"] = btc\ntrain_X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import List, Set, Dict, Tuple\nfrom collections import defaultdict\nimport operator\n\ndef get_list_of_dicts_col(col_name: str, dict_key: str= None) -> List[List]:\n    if dict_key is None:\n        return [ast.literal_eval(i) for i in train_input[col_name] if type(i) == str]\n    else:\n        return [[x[dict_key] for x in ast.literal_eval(i)] for i in train_input[col_name] if type(i) == str]\n\ndef get_sorted_unique_value_count_in_list_of_lists(list_of_lists: str or List[List], col_name: str = None, dict_key: str= None) -> List[Tuple]:\n    item_count = defaultdict(int)\n    \n    if type(list_of_lists) == str:\n        assert col_name is not None and dict_key is not None, \"specify the column name dict key for which to count unique values\"\n        list_of_lists = get_list_of_dicts_col(col_name, dict_key)\n\n    for list_ in list_of_lists:\n        for item in list_:\n            item_count[item] += 1\n\n    return sorted(item_count.items(), key=operator.itemgetter(1), reverse=True)\n\ndef unzip_tuples(list_of_tuples: List[Tuple]) -> List[List]:\n    return [list(i) for i in list(list(zip(*list_of_tuples)))]\n\ndef get_items_with_minimum_percentage(percentage: float, list_of_tuples: List[Tuple], col_name: str) -> List[Tuple]:\n    return [i for i in list_of_tuples if i[1]/len(train_input[col_name]) > percentage]\n\ndef get_one_hot_df(columns: List, list_of_item_lists: List[List], column_name_prepend: str) -> pd.DataFrame:\n    one_hot_df = pd.DataFrame(columns=[column_name_prepend + str(i) for i in columns])\n\n    for item_list in list_of_item_lists:\n        row = []\n        for item in columns:\n            if item in item_list:\n                row.append(1)\n            else:\n                row.append(0)\n            \n        one_hot_df = one_hot_df.append(pd.DataFrame([row], columns=[column_name_prepend + str(i) for i in columns]), ignore_index=True)\n    return one_hot_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### genres"},{"metadata":{"trusted":true},"cell_type":"code","source":"genre_col = get_list_of_dicts_col(\"genres\", \"id\")\n\ngenre_count_tuples = get_sorted_unique_value_count_in_list_of_lists(genre_col)\n        \nprint(\"Number of unique genres: %d\" % len(genre_count_tuples))\n\ngenre_list = unzip_tuples(genre_count_tuples)[0]\nprint(genre_list)\n\none_hot_genre = get_one_hot_df(genre_list, genre_col, \"genre_\")\ntrain_X = pd.concat([train_X.drop(columns=\"genres\"), one_hot_genre], axis=1)\ntrain_X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### production_countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"country_col = get_list_of_dicts_col(\"production_countries\", \"iso_3166_1\")\ncountry_count_tuples = get_sorted_unique_value_count_in_list_of_lists(country_col)\n    \nprint(\"Number of unique countries: %d\" % len(country_count_tuples))\n\ncountries_list = unzip_tuples(country_count_tuples)[0]\ncountries_counts_list = unzip_tuples(country_count_tuples)[1]\n\nplt.figure(figsize=(20,8))\nplt.bar(countries_list[:40], countries_counts_list[:40])\nplt.title(\"Top 40 Countries\")\nplt.show()\n\n\ncountry_count_tuples = get_items_with_minimum_percentage(0.01, country_count_tuples, \"production_countries\")\n\ncountries_list = unzip_tuples(country_count_tuples)[0]\ncountries_counts_list = unzip_tuples(country_count_tuples)[1]\n\nprint(\"Countries that make up at least \" + str(percentage_required * 100) + \"% of the data \" + str(countries_list))\n\none_hot_country = get_one_hot_df(countries_list, country_col, \"country_\")\n\ntrain_X = pd.concat([train_X.drop(columns=\"production_countries\"), one_hot_country], axis=1)\ntrain_X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Production Companies"},{"metadata":{"trusted":true},"cell_type":"code","source":"company_col = get_list_of_dicts_col(\"production_companies\", \"id\")\ncompany_count_tuples = get_sorted_unique_value_count_in_list_of_lists(company_col)\n    \nprint(\"Number of unique companies: %d\" % len(company_count_tuples))\n\ncompanies_list = unzip_tuples(company_count_tuples)[0]\ncompanies_counts_list = unzip_tuples(company_count_tuples)[1]\n\nplt.figure(figsize=(20,8))\nplt.bar([str(i) for i in companies_list[:40]], companies_counts_list[:40])\nplt.title(\"Top 40 Companies\")\nplt.show()\n\ncompany_count_tuples = get_items_with_minimum_percentage(0.01, company_count_tuples, \"production_companies\")\n\ncompanies_list = unzip_tuples(company_count_tuples)[0]\ncompanies_counts_list = unzip_tuples(company_count_tuples)[0]\n\nprint(\"Companies that make up at least \" + str(percentage_required * 100) + \"% of the data \" + str(companies_list))\n\none_hot_company = get_one_hot_df(companies_list, company_col, \"company_\")\n\ntrain_X = pd.concat([train_X.drop(columns=\"production_companies\"), one_hot_company], axis=1)\ntrain_X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords_col = get_list_of_dicts_col(\"Keywords\", \"id\")\nkeywords_count_tuples = get_sorted_unique_value_count_in_list_of_lists(keywords_col)\n    \nprint(\"Number of unique keywords: %d\" % len(keywords_count_tuples))\n\nkeywords_list = unzip_tuples(keywords_count_tuples)[0]\nkeywords_counts_list = unzip_tuples(keywords_count_tuples)[1]\n\nplt.figure(figsize=(20,8))\nplt.bar([str(i) for i in keywords_list[:40]], keywords_counts_list[:40])\nplt.title(\"Top 40 Keywords\")\nplt.show()\n\nkeywords_count_tuples = get_items_with_minimum_percentage(0.02, keywords_count_tuples, \"Keywords\")\n\nkeywords_list = unzip_tuples(keywords_count_tuples)[0]\nkeywords_counts_list = unzip_tuples(keywords_count_tuples)[1]\n\nprint(\"Number of keywords used for one hot encoding: \" + str(len(keywords_list)))\n\none_hot_keywords = get_one_hot_df(keywords_list, keywords_col, \"keyword_\")\n\ntrain_X = pd.concat([train_X.drop(columns=\"Keywords\"), one_hot_keywords], axis=1)\ntrain_X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poster_url_head = \"https://image.tmdb.org/t/p/w600_and_h900_bestv2\"\nfrom functools import reduce\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(131)\nplt.hist(train_X[\"budget\"], bins=np.linspace(0, 3e8, 50))\nplt.title(\"Budget\")\n\nplt.subplot(132)\nplt.hist(train_Y, bins=np.linspace(0, 3.5e8, 50))\nplt.title(\"Revenue\")\n\nplt.subplot(133)\nplt.hist(train_X[\"popularity\"], bins=np.linspace(0, 200, 100))\nplt.title(\"Popularity\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(len([i for i in zip(train[\"original_title\"], train_X[\"budget\"]) if i[1] != 0 and i[1] < 700]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import ast\n\n# genres = [ast.literal_eval(i) for i in train[\"genres\"] if type(i) == str]\nprint([i[1] for i in zip(train_X[\"genres\"], train_X[\"original_title\"]) if type(i[0]) != str])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import seaborn as sns\n# \n# colormap = plt.cm.RdBu\n# plt.figure(figsize=(14,12))\n# plt.title('Pearson Correlation of Features', y=1.05, size=15)\n# sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n#             square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}